---
title: Exllama v2 (GPTQ and EXL2)
---

ExLlamaV2 is an inference library for running local LLMs on modern consumer GPUs.

https://github.com/turboderp/exllamav2

## Example

:::warning
Please make sure to change syntax to `#syntax=ghcr.io/sozercan/snapgen:latest` in the examples below.
:::

### EXL2
https://github.com/sozercan/snapgen/blob/main/test/snapgenfile-exllama2-exl2.yaml

### GPTQ
https://github.com/sozercan/snapgen/blob/main/test/snapgenfile-exllama2-gptq.yaml
